   Compiling stroki v0.1.0 (/home/aa/Документы/Rust/stroki)
warning: unused import: `HashMap`
 --> src/lib.rs:2:44
  |
2 | use std::collections::{BTreeMap, BTreeSet, HashMap};
  |                                            ^^^^^^^
  |
  = note: `#[warn(unused_imports)]` on by default

warning: function is never used: `stupid_alphabet`
   --> src/lib.rs:148:4
    |
148 | fn stupid_alphabet() -> impl Iterator<Item = String> {
    |    ^^^^^^^^^^^^^^^
    |
    = note: `#[warn(dead_code)]` on by default

warning: function is never used: `alphabeticall_sort`
   --> src/lib.rs:153:4
    |
153 | fn alphabeticall_sort(
    |    ^^^^^^^^^^^^^^^^^^

warning: `stroki` (lib) generated 3 warnings
    Finished release [optimized] target(s) in 1.35s
     Running `target/release/stroki tsts/future-generations.json tsts/future-generations.md`
[src/lib.rs:57] &old_array_hashset = {
    (
        "future-generations:033",
        "If you’re already on our newsletter, email us at [book.giveaway@80000hours.org](/cdn-cgi/l/email-protection#0e6c616165206967786b6f796f77286d6163636f7a35363e3e3e3e66617b7c7d20617c69) to get a copy.",
    ),
    (
        "future-generations:037",
        "</div>",
    ),
    (
        "future-generations:039",
        "If you prefer to start with a video, this presentation by Joseph Carlsmith is also a good introduction:",
    ),
    (
        "future-generations:041",
        "!ytb-Ccq2Ql8FcY0",
    ),
    (
        "future-generations:043",
        "<div class=\"panel clearfix \">",
    ),
    (
        "future-generations:079",
        "</div>",
    ),
    (
        "future-generations:267",
        "</div>",
    ),
    (
        "future-generations:271",
        "- [Moral uncertainty: how to act when you’re uncertain about what’s good](https://80000hours.org/articles/moral-uncertainty/)\n- Chapters 1-4 [_On the Overwhelming Importance of Shaping the Far Future_](https://80000hours.org/wp-content/uploads/2022/01/Beckstead-Nick-On-the-Overwhelming-Importance-of-Shaping-the-Far-Future-better-formatting.pdf) by Nick Beckstead. This thesis builds upon a seminal paper by Nick Bostrom, [_Astronomical Waste_](https://nickbostrom.com/astronomical/waste.html), which makes the argument that the future has overwhelming value for [utilitarians](https://www.utilitarianism.net/), as well as [_Existential Risk Prevention as a Global Priority_](http://www.existential-risk.org/concept.html).\n- A more recent and systematic academic treatment of the topic can be found here: [The Case for Strong Longtermism](https://globalprioritiesinstitute.org/hilary-greaves-william-macaskill-the-case-for-strong-longtermism-2/), by Hilary Greaves and Will MacAskill\n- Dr Nick Beckstead also discusses his thesis [in our podcast](https://80000hours.org/2017/10/nick-beckstead-giving-billions/).\n- Dr Toby Ord discusses these arguments in his book, [_The Precipice_](https://theprecipice.com/)\n- Dr Toby Ord also explores many of these arguments with us in another [podcast](https://80000hours.org/articles/why-the-long-run-future-matters-more-than-anything-else-and-what-we-should-do-about-it/).\n- Benjamin Todd and Arden Koehler discuss varieties of longtermism in [this podcast](https://80000hours.org/podcast/episodes/ben-todd-on-varieties-of-longtermism/).\n- [Our podcast with Alexander Berger](https://80000hours.org/podcast/episodes/alexander-berger-improving-global-health-wellbeing-clear-direct-ways/) presents a case for focusing instead on near-term gains in global health and wellbeing.\n- \u{200b}\u{200b}\u{200b}\u{200b}In this podcast, Holden Karnofsky talks about the case that we’re living in [the most important century](https://80000hours.org/podcast/episodes/holden-karnofsky-most-important-century/). And in a blog post, [_Are we living at the most influential time in history?_](https://forum.effectivealtruism.org/posts/XXLf6FmWujkxna3E6/are-we-living-at-the-most-influential-time-in-history-1), Dr Will MacAskill responds with counterarguments.\n- [This could be the most important century](https://80000hours.org/articles/the-most-important-century/)\n- [Why despite global progress, humanity is probably facing its most dangerous time ever.](https://80000hours.org/articles/extinction-risk/)\n- [Fifty-one policy and research ideas](https://80000hours.org/2020/04/longtermist-policy-ideas/) for reducing existential risk\n- [Carl Shulman on the common-sense case for existential risk work and its practical implications](https://80000hours.org/podcast/episodes/carl-shulman-common-sense-case-existential-risks/)\n- An alternative [introduction to longtermism](https://www.effectivealtruism.org/articles/longtermism/), which goes through somewhat different objections and reasoning",
    ),
    (
        "future-generations:290",
        "<div class=\"tw--grid xs:tw--grid-flow-col tw--gap-3\"><div class=\"xs:tw--order-last tw--pt-1\">\n[![](https://80000hours.org/wp-content/uploads/2017/03/cost-effectiveness-of-health-interventionsV1-01-e1489184505545-720x448.png)](https://80000hours.org/articles/effective-altruism/)\n</div>",
    ),
    (
        "future-generations:298",
        "</div>",
    ),
    (
        "future-generations:304",
        "</div>",
    ),
    (
        "future-generations:306",
        "</div>",
    ),
    (
        "future-generations:308",
        "</div>",
    ),
    (
        "future-generations:310",
        "<div class=\"well bg-gray-lighter margin-bottom margin-top padding-top-small padding-bottom-small\">",
    ),
    (
        "future-generations:316",
        "!newsletter-subscribe",
    ),
    (
        "future-generations:318",
        "</div>",
    ),
    (
        "future-generations:323",
        "</div>",
    ),
}
[src/lib.rs:58] &new_string_hashset = {
    "!ytb-LEENEFaVUzU",
    "(Also, if you’re visiting us _from_ this video — welcome! We hope you like what you read.)",
    "- Chapters 1-4 [_On the Overwhelming Importance of Shaping the Far Future_](https://80000hours.org/wp-content/uploads/2022/01/Beckstead-Nick-On-the-Overwhelming-Importance-of-Shaping-the-Far-Future-better-formatting.pdf) by Nick Beckstead (also discussed [in a podcast episode with us](https://80000hours.org/2017/10/nick-beckstead-giving-billions/)). This thesis builds upon a seminal paper by Nick Bostrom, [_Astronomical Waste_](https://nickbostrom.com/astronomical/waste.html), which makes the argument that the future has overwhelming value for [utilitarians](https://www.utilitarianism.net/), as well as [_Existential Risk Prevention as a Global Priority_](http://www.existential-risk.org/concept.html).\n- A more recent and systematic academic treatment of the topic can be found here: [The Case for Strong Longtermism](https://globalprioritiesinstitute.org/hilary-greaves-william-macaskill-the-case-for-strong-longtermism-2/), by Hilary Greaves and Will MacAskill\n- Dr Toby Ord discusses these arguments in his book, [_The Precipice_](https://theprecipice.com/), and in a [podcast episode with us](https://80000hours.org/articles/why-the-long-run-future-matters-more-than-anything-else-and-what-we-should-do-about-it/).\n- Benjamin Todd and Arden Koehler discuss varieties of longtermism in [this podcast](https://80000hours.org/podcast/episodes/ben-todd-on-varieties-of-longtermism/).\n- [Our podcast with Alexander Berger](https://80000hours.org/podcast/episodes/alexander-berger-improving-global-health-wellbeing-clear-direct-ways/) presents a case for focusing instead on near-term gains in global health and wellbeing.\n- \u{200b}\u{200b}\u{200b}\u{200b}In this podcast, Holden Karnofsky talks about the case that we’re living in [the most important century](https://80000hours.org/podcast/episodes/holden-karnofsky-most-important-century/). And in a blog post, [_Are we living at the most influential time in history?_](https://forum.effectivealtruism.org/posts/XXLf6FmWujkxna3E6/are-we-living-at-the-most-influential-time-in-history-1), Dr Will MacAskill responds with counterarguments.\n- [Why despite global progress, humanity is probably facing its most dangerous time ever.](https://80000hours.org/articles/extinction-risk/)\n- [Carl Shulman on the common-sense case for existential risk work and its practical implications](https://80000hours.org/podcast/episodes/carl-shulman-common-sense-case-existential-risks/)\n- An alternative [introduction to longtermism](https://www.effectivealtruism.org/articles/longtermism/), which goes through somewhat different objections and reasoning\n- Introductory overviews on longtermism from the [Forethought Foundation for Global Priorities Research](https://www.forethought.org/longtermism), [Our World In Data](https://ourworldindata.org/longtermism), [Wikipedia](https://en.wikipedia.org/wiki/Longtermism), and [1000 Word Philosophy](https://1000wordphilosophy.com/2021/09/17/longtermism/).\n- [Will MacAskill on whether you have to buy longtermism to prioritise reducing existential risk](https://80000hours.org/podcast/episodes/will-macaskill-ambition-longtermism-mental-health/)\n- If you’re interested in making charitable donations, see [Giving What We Can’s recommended charities and funds that are safeguarding the long-term future.](https://www.givingwhatwecan.org/cause-areas/long-term-future)\n- [Moral uncertainty: how to act when you’re uncertain about what’s good](https://80000hours.org/articles/moral-uncertainty/)",
    "<div class=\"tw--grid xs:tw--grid-flow-col tw--gap-3\"><div class=\"xs:tw--order-last tw--pt-1\">\n[!img~ width=\"720\" height=\"448\" ~[Decorative post preview](https://80000hours.org/wp-content/uploads/2017/03/cost-effectiveness-of-health-interventionsV1-01-e1489184505545-720x448.png)](https://80000hours.org/articles/effective-altruism/)\n</div>",
    "If you prefer to start by watching something, this video by Kurzgesagt presents the idea that there could be many future generations with very good lives. We’ll explain what this implies in the rest of this article.",
    "If you’re already on our newsletter, email us at [book.giveaway@80000hours.org](/cdn-cgi/l/email-protection#b1d3dededa9fd6d8c7d4d0c6d0c897d2dedcdcd0c58a8981818181d9dec4c3c29fdec3d6) to get a copy.",
}
[src/lib.rs:59] &unsorted_partial_answer = [
    (
        "future-generations:000",
        "# Longtermism: the moral significance of future generations\n## !empty\n<p class=\"entry-meta small\">By <span class=\"byline author vcard\"><a href=\"https://80000hours.org/author/benjamin-todd/\" rel=\"author\" class=\"fn\">Benjamin Todd</a></span> · Published <time class=\"published\" datetime=\"2017-10-24T06:33:52+00:00\">October 2017</time></p>",
    ),
    (
        "future-generations:004",
        "!podcast-player",
    ),
    (
        "future-generations:006",
        "Most people think we should have some concern for future generations, but this obvious sounding idea can lead to a surprising conclusion.",
    ),
    (
        "future-generations:008",
        "Since the future might be very long, there could be far more people in future generations than in the present generation. This means that if you want to make the world better in an _impartial_ way — i.e. without regard to people’s race, class, or where or _when_ they’re born — then what most matters morally is that the future goes as well as it can for all generations to come. We’ve called this the ‘long-term value thesis.’",
    ),
    (
        "future-generations:010",
        "When this thesis is also combined with the idea that some of our actions can have non-negligible effects on how the future goes, it implies that one of our biggest priorities should be ensuring the future goes well. This further idea is usually called ‘longtermism.’ Most of this article is about the long-term value thesis, but it comes back to whether we can affect the future at the end.",
    ),
    (
        "future-generations:012",
        "The long-term value thesis is often confused with the claim that we shouldn’t _do_ anything to help people in the present generation. But the thesis is about what most _matters_ — what we should _do_ about it is a further question. If it turned out that the best way to help those in the future is to improve the lives of people in the present, such as through providing health and education, then longtermists would focus on that. The difference is that the biggest _reason_ to help those in the present would be to improve the long term.",
    ),
    (
        "future-generations:014",
        "The arguments for and against longtermism are a fascinating new area of research. Many of the key advances have been made by philosophers who have spent time in Oxford, like [Derek Parfit](https://www.amazon.com/Reasons-Persons-Derek-Parfit/dp/019824908X), [Nick Bostrom](https://nickbostrom.com/astronomical/waste.html), [Nick Beckstead](http://www.nickbeckstead.com/research), [Hilary Greaves](http://users.ox.ac.uk/~mert2255/#research) and [Toby Ord](http://www.tobyord.com/research/). We’ve found it interesting to watch them deepen and refine these arguments over the last 10 or so years, and we think longtermism might well turn out to be one of the most important discoveries of [effective altruism](https://www.centreforeffectivealtruism.org/what-is-effective-altruism/) so far.",
    ),
    (
        "future-generations:016",
        "In the rest of this article, you can see a one-page introduction to longtermism in a nutshell, then we give an overview of the main arguments for and against the long-term value thesis, discuss three common objections, and finish by briefly discussing whether we can affect the future and what longtermism might imply.",
    ),
    (
        "future-generations:018",
        "<div class=\"panel clearfix \">",
    ),
    (
        "future-generations:020",
        "[!img~ style=\"width: 30%; margin-left: 10%; margin-top:-10px; float: right;\" ~[book cover](https://80000hours.org/wp-content/uploads/2020/03/the-precipice-3d.jpg)](/the-precipice/)",
    ),
    (
        "future-generations:022",
        "<div style=\"margin-top: -10px;\">\nIf you prefer a book, Dr Toby Ord, an Oxford philosopher and 80,000 Hours trustee, has recently published _[The Precipice: Existential Risk and the Future of Humanity](/the-precipice/)_ which gives an overview of the moral importance of future generations, and what we can do to help them today.",
    ),
    (
        "future-generations:025",
        "#### We’ll mail you the book (for free)",
    ),
    (
        "future-generations:027",
        "Join the 80,000 Hours newsletter and we’ll send you a free copy of the book.",
    ),
    (
        "future-generations:029",
        "We’ll also send you updates on our latest research, opportunities to work on existential risk, and news from the author.",
    ),
    (
        "future-generations:031",
        "!newsletter-subscribe",
    ),
    (
        "",
        "",
    ),
    (
        "future-generations:035",
        "</div>",
    ),
    (
        "",
        "",
    ),
    (
        "",
        "",
    ),
    (
        "",
        "",
    ),
    (
        "",
        "",
    ),
    (
        "",
        "",
    ),
    (
        "future-generations:045",
        "## Longtermism in a nutshell",
    ),
    (
        "future-generations:047",
        "The average mammalian species lasts for about one million years. Homo sapiens have been around for only 200,000. With the benefit of technology and foresight, civilisation could, in principle, survive for at least as long as the earth is habitable — probably around 600–800 million years more.[^:`And if humanity finds a way to flourish outside of Earth, civilisation could last far longer still.`]",
    ),
    (
        "future-generations:049",
        "Given that we can’t rule out this possibility, this means that there will, [in expectation](https://80000hours.org/articles/expected-value/), be a huge number of future generations. There could also be a much larger number of people in each future generation, and their lives could be much better than ours.",
    ),
    (
        "future-generations:051",
        "We think future generations clearly matter, and impartial concern most likely implies their interests matter as much as anyone’s.[^:`Technically, another claim is needed to complete this argument. We must say that others' interests matter regardless of their 'modal status' -- roughly whether an individual would exist 'no matter what we do' or 'might exist or might not exist depending on our actions.'",
    ),
    (
        "future-generations:053",
        "People who believe that this is an important moral difference often hold a 'person-affecting view.' We've given some reasons we disagree with the person-affecting view in our [article on future generations](https://80000hours.org/articles/future-generations/#3-do-we-have-moral-obligations-to-future-generations).`]",
    ),
    (
        "future-generations:055",
        "If we care about _all_ the consequences of our actions, then what’s most important about our actions from an impartial perspective is their potential effects on these future generations.",
    ),
    (
        "future-generations:057",
        "If this reasoning is correct, it would imply that approaches to improving the world should be evaluated mainly in terms of their potential long-term impact, over thousands, millions, or even billions of years.",
    ),
    (
        "future-generations:059",
        "In other words, the question “How can I have a positive impact?” should mostly be replaced with “How can I best make the very long-term future go well?” These arguments and their implications are studied as part of an emerging school of thought called _longtermism_.",
    ),
    (
        "future-generations:061",
        "We feel relatively confident about this idea, but we’re not confident about what it implies in practice.",
    ),
    (
        "future-generations:063",
        "An obvious response to the above is that it’s so difficult to predict the very long-term effects of our actions that although these effects might be very important, we don’t know what they are. Instead, this response goes, we should focus on helping people in the short-term where we can have more confidence in their positive effects.",
    ),
    (
        "future-generations:065",
        "We agree it’s very hard to know the long-term effects of our actions; however, as discussed above, we think we should aim to use whatever evidence and theory is available to make the best possible estimates of the expected value of different actions. Moreover, because the expected number of future generations is so great, our actions only need to have non-negligible effects on them for these effects to dominate their expected value.",
    ),
    (
        "future-generations:067",
        "In practice, we think there _are_ some actions that potentially have very long-term positive effects. For example, we can take steps to make it less likely that civilisation ends through a disaster like nuclear war, which would irreversibly deprive future generations of the chance to flourish. We cover other examples in the next section.",
    ),
    (
        "future-generations:069",
        "Let’s explore some hypothetical numbers to illustrate the general concept. If there’s a 5% chance that civilisation lasts for ten million years, then in expectation, there are over 5,000 future generations. If thousands of people making a concerted effort could, with a 55% probability, reduce the risk of premature extinction by 1 percentage point, then these efforts would in expectation save 28 future generations. If each generation contains ten billion people, that would be 280 billion additional individuals who get to live flourishing lives. If there’s a chance civilisation lasts longer than ten million years, or that there are more than ten billion people in each future generation, then the argument is strengthened even further.",
    ),
    (
        "future-generations:071",
        "Even if we’re not sure what actions would help today, longtermism would likely imply that our key focus should be on carrying out research to identify these actions, or otherwise making society better able to tackle long-term challenges.",
    ),
    (
        "future-generations:073",
        "In contrast, we don’t see much reason to expect that actions with good short-term effects will also be those that will be best from a long-term perspective.",
    ),
    (
        "future-generations:075",
        "Another major reason why we think it’s worth taking longtermism seriously is that future generations lack any economic or political power, which means we should expect their interests to be neglected by our current institutions. Within philanthropy, too, very little attention is paid to the interests of those who might live more than 100 years in the future. This all suggests that outstanding opportunities to help may remain untaken, and that it would be reasonable for society to allocate significantly more attention to these issues.",
    ),
    (
        "future-generations:077",
        "We remain unsure about many of these arguments, but overall we’re persuaded that focusing more on the very long-term effects of our actions is our best bet for now.",
    ),
    (
        "",
        "",
    ),
    (
        "future-generations:081",
        "## Why think the future matters more than the present?",
    ),
    (
        "future-generations:083",
        "What are the things you most value in human civilisation today? People being happy and free of suffering? People fulfilling their potential? Knowledge? Art?",
    ),
    (
        "future-generations:085",
        "In almost all of these cases, there’s potentially a lot more of it to come in the future:",
    ),
    (
        "future-generations:087",
        "1. The Earth could remain habitable for 600–800 million years,[^:`800 million years from now:",
    ),
    (
        "future-generations:089",
        "   > Carbon dioxide levels fall to the point at which C4 photosynthesis is no longer possible. Free oxygen and ozone disappear from the atmosphere. Multicellular life dies out.",
    ),
    (
        "future-generations:091",
        "   _Timeline of the far future_, Wikipedia,",
    ),
    (
        "future-generations:093",
        "   [Archived link](https://web.archive.org/web/20171022015521/https://en.wikipedia.org/wiki/Timeline_of_the_far_future), retrieved 22-October-2017`] so there could be about 21 million future generations,[^:`Assuming 3 generations per 100 years.`] and if we do the needed work, they could lead great lives — whatever you think ‘great’ consists of. Even if you don’t think future generations matter as much as the present generation, since there could be so many of them, they could still be our key concern.",
    ),
    (
        "future-generations:095",
        "2. Civilisation could also eventually reach other planets — there are 100 billion planets in the Milky Way alone.[^:`_The Milky Way Contains at Least 100 Billion Planets According to Survey_, Hubblesite,",
    ),
    (
        "future-generations:097",
        "   [Archived link](https://web.archive.org/web/20140723213047/http://hubblesite.org/newscenter/archive/releases/2012/07/full/), retrieved 22-October-2017`] So, even if there’s only a small chance of this happening, there could also be dramatically more people _per generation_ than there are today. By reaching other planets, civilisation could also last even longer than if we stay on the Earth.",
    ),
    (
        "future-generations:099",
        "3. If you think it’s good for people to live happier and more flourishing lives, there’s a possibility that technology and social progress will let people have much better and longer lives in the future (including people in the present generation). So, putting these first three points together, there could be many more generations, with far more people, with the potential to be living much better lives. The three dimensions multiply together to give the potential scale of the future.",
    ),
    (
        "future-generations:101",
        "4. It seems possible that the future could contain far more of the other things humans value, including beauty, justice, and knowledge.[^:`Though some forms of justice and virtue focused ethics might not hold that we ought to _maximise_ justice or virtue; instead, for instance, it may be a matter of satisfying a set of conditions.`]",
    ),
    (
        "future-generations:103",
        "5. If you greatly value artistic and intellectual achievement, a far wealthier and bigger civilisation could have far greater achievements than our own.",
    ),
    (
        "future-generations:106",
        "\nAnd so on.",
    ),
    (
        "future-generations:108",
        "This suggests that, insofar as you care about making the world a better place, your key concern should be whether the future goes well or badly.",
    ),
    (
        "future-generations:110",
        "This isn’t to deny that you have special obligations to your friends and family, and an interest in your own life going well. We’re only talking about what matters insofar as you care about helping others impartially — philosophers often call this what matters “from the point of view of the universe.” We think everyone should care about the lives of all others in this sense _to some degree_, even if you care about other things as well.",
    ),
    (
        "future-generations:112",
        "People often assume the long-term value thesis is especially about the possibility of there being lots of _people_ in the future, and so only of interest to a narrow range of ethical views (especially [total utilitarianism](http://users.ox.ac.uk/~mert2255/papers/population_axiology.pdf)), but as we can see in the list above, it’s actually much broader. It just rests on the idea that if _something_ is of value, it’s better to have more of what’s valuable rather than less, and that it’s possible to have much more of it in the future. This might include non-welfare values, such as beauty or knowledge. The arguments are also not about humans; rather, they concern whatever agents in the future might have moral value, including other species.",
    ),
    (
        "future-generations:114",
        "People also often think that the long-term value thesis assumes the future will have positive rather than negative value. Quite the opposite is true — the future could also contain far more suffering than the present, and this implies _even more_ concern for how it unfolds. It’s important to reduce the probability of bad futures as well as increase the probability of good ones.",
    ),
    (
        "future-generations:116",
        "You can see a more rigorous presentation of the arguments in Chapter 3 of [_On the Overwhelming Importance of Shaping the Far Future_](https://80000hours.org/wp-content/uploads/2022/01/Beckstead-Nick-On-the-Overwhelming-Importance-of-Shaping-the-Far-Future-better-formatting.pdf), by Nick Beckstead.",
    ),
    (
        "future-generations:118",
        "Now let’s consider three of the most common objections to the long-term value thesis.",
    ),
    (
        "future-generations:120",
        "## 1\\. Will the future actually be big?",
    ),
    (
        "future-generations:122",
        "The argument relies on the possibility of there being much more value in the future. But you might doubt that civilisation can actually survive very long, or that we will ever live on other planets, or that people’s lives can be much better or worse than those of people today.",
    ),
    (
        "future-generations:124",
        "There’s a lot of reason to doubt these claims. Let’s look at them in a little more depth.",
    ),
    (
        "future-generations:126",
        "First, what’s _not_ up for debate is the _possibility_ that the future could be big. It’s a widely accepted scientific position that the Earth could remain habitable for hundreds of millions of years, and that there are at least a hundred billion planets in the galaxy. What’s more, there’s no reason to think it’s impossible that civilisation could discover far more powerful technology than we have today, or that people could live far better and more satisfying lives than they do today.",
    ),
    (
        "future-generations:128",
        "Rather, what’s in doubt is the _likelihood_ that these developments come to pass. Unfortunately there is no definitive way to estimate this likelihood. The best we can do is to weigh the arguments for and against a big future, and make our best estimates.",
    ),
    (
        "future-generations:130",
        "If you think that civilisation is virtually guaranteed to end in the next couple of hundred years, then the future won’t have much more value than the present. However, if you think there’s a 5% chance that civilisation survives 10 million generations till the end of the Earth,[^:`Sometimes people object that this is a \"Pascal's wager\" type argument, but a 10% chance is typically larger than is used in these arguments, and the supposed future value is still finite rather than infinite.`] then (in expectation) there will be over 500,000 future generations. This means the future is at least 500,000 times ‘bigger’ than the present. This could happen if there’s a chance civilisation reaches a stable state where the risk of extinction becomes low.",
    ),
    (
        "future-generations:132",
        "In general, the bigger you think the future could be, and the more likely you think we are to get there, the greater the value.",
    ),
    (
        "future-generations:134",
        "Further, if you’re _uncertain_ whether the future will be big, then a top priority should be to _figure out_ whether it will be — it would be the most important moral discovery you could make. So, even if you’re not sure you directly should act on the thesis, it might still be the most important area for research. We see this kind of research as extremely important.",
    ),
    (
        "future-generations:136",
        "## 2\\. What about discounting?",
    ),
    (
        "future-generations:138",
        "Sometimes at this point people, especially those trained in economics, mention “discounting” as a reason to not care about the long term.",
    ),
    (
        "future-generations:140",
        "When economists compare benefits in the future to benefits in the present, they typically reduce the value of the future benefits by some amount called the ‘discount factor.’ A typical social discount rate might be 1% per year, which means that benefits in 100 years are only worth 36% as much as benefits today, and benefits in 1,000 years are worth almost nothing.",
    ),
    (
        "future-generations:142",
        "To understand whether this is a valid response, you need to consider why the concept of discounted benefits was invented in the first place.",
    ),
    (
        "future-generations:144",
        "There are good reasons to discount _economic_ benefits. One reason is that if you receive money now, you can invest it, and earn a return each year. This means it’s better to receive money now rather than later. People in the future might also be wealthier, which means that money is less valuable to them.",
    ),
    (
        "future-generations:146",
        "However, these reasons don’t obviously apply to welfare — people having good lives. You can’t directly “invest” welfare today and get more welfare later, like you can with money. The same seems true for other intrinsic values, such as justice.",
    ),
    (
        "future-generations:148",
        "There are other reasons to discount welfare,[^:`For instance, we might discount welfare due to uncertainty about whether it will happen, but we've already taken uncertainty about the future into account when estimating its expected value.",
    ),
    (
        "future-generations:150",
        "We might also discount welfare in practice, since having happy people now might be thought to produce more happy people in the future.`] and this is a complex debate. However, the bottom line is that almost every philosopher who has worked on the issue doesn’t think we should discount the _intrinsic value_ of welfare — i.e. from the point of view of the universe, one person’s happiness is worth just the same amount no matter when it occurs.",
    ),
    (
        "future-generations:152",
        "Indeed, if you suppose we can discount welfare, we can easily end up with conclusions that sound absurd. For instance, a 3% discount rate would imply that the suffering of one person today was equal to the suffering of 16 trillion people in 1,000 years.",
    ),
    (
        "future-generations:154",
        "As Derek Parfit said:",
    ),
    (
        "future-generations:156",
        "> Why should costs and benefits receive less weight, simply because they are further in the future? When the future comes, these benefits and costs will be no less real. Imagine finding out that you, having just reached your twenty-first birthday, must soon die of cancer because one evening Cleopatra wanted an extra helping of dessert. How could this be justified?",
    ),
    (
        "future-generations:158",
        "If we reject the discounting of welfare and other intrinsic values, then the chance that there could be a great deal of value in the future is still important. Moreover, this doesn’t stand in tension with the economic practice of discounting monetary benefits.",
    ),
    (
        "future-generations:160",
        "If you’d like to see a more technical discussion of these issues, see [_Discounting for Climate Change_](http://users.ox.ac.uk/~mert2255/papers/discounting.pdf) by Hilary Graves. There is a more accessible discussion at 1h00m50s in [our podcast with Toby Ord](https://80000hours.org/articles/why-the-long-run-future-matters-more-than-anything-else-and-what-we-should-do-about-it/) and in Chapter 4 of [_Stubborn Attachments_](https://web.archive.org/web/20170808184525/medium.com/stubborn-attachments/stubborn-attachments-full-text-8fc946b694d) by Tyler Cowen.",
    ),
    (
        "future-generations:162",
        "## 3\\. Do we have moral obligations to future generations?",
    ),
    (
        "future-generations:164",
        "A final response is that although the future might be big, we’re not obligated to help people who don’t yet exist in the same way as we’re obligated to help people alive right now.",
    ),
    (
        "future-generations:166",
        "This objection is usually associated with a “person-affecting” view of ethics, which is sometimes summed up as the view that “Ethics is about helping make people happy, not making happy people.” In other words, we only have moral obligations to help those who are already alive, and not to enable more people to exist with good lives.",
    ),
    (
        "future-generations:168",
        "You can see where this intuition comes from if you consider the following choice: is it better to cure one person who’s 60 years old of cancer and allow them to live to 80, or to bring one new person into existence who will live a good life for 80 years? Most people think we should help the 60 year old, even though the new person gains four times as much good life.",
    ),
    (
        "future-generations:170",
        "However, person-affecting views suffer from a number of problems. For instance, suppose you have the option to bring into existence someone who would not otherwise have existed, whose life involves severe and constant suffering from birth until death, and who wished they had never been born.",
    ),
    (
        "future-generations:172",
        "Nearly everyone agrees this is a bad thing to do. A naive person-affecting view, however, says that, since it involves creating a new person, this lies outside of our ethical concern, and so is neither good nor bad. So, the person-affecting view conflicts with the obvious idea that we shouldn’t create the suffering life.",
    ),
    (
        "future-generations:174",
        "Person-affecting views can avoid this conflict by positing that it’s bad to create lives filled with suffering, but it’s neither good nor bad to create _happy_ lives. Then, it’s wrong to create the suffering-filled life, but there’s no reason to enable more happy people to exist in the future.",
    ),
    (
        "future-generations:176",
        "One issue with this is that it’s unclear why this asymmetry would exist. The bigger problem though is that this asymmetry conflicts with another common sense idea.",
    ),
    (
        "future-generations:178",
        "Suppose you have the choice to bring into existence one person with an amazing life, or another person whose life is barely worth living, but still more good than bad. Clearly, it seems better to bring about the amazing life, but if creating a happy life is neither good nor bad, then we have to conclude that both options are neither good nor bad. This implies both options are equally good, which seems bizarre.",
    ),
    (
        "future-generations:180",
        "This is a complex debate, and rejecting the person-affecting view also has counterintuitive conclusions. For instance, if you agree that it’s good to create people whose lives are more good than bad, then you’ll need to accept that we could have a better world filled with a huge number of people whose lives are just barely worth living. This is called the “ [repugnant conclusion](https://plato.stanford.edu/entries/repugnant-conclusion/).”",
    ),
    (
        "future-generations:182",
        "Our view, however, is that it’s better to reject the person-affecting view. You can see a summary of the arguments in this [public lecture by Hilary Greaves](https://www.youtube.com/watch?v=0cHT4yWUEaA) (based on [this paper](http://users.ox.ac.uk/~mert2255/papers/population_axiology.pdf)) and in Chapter 4 of [_On the Overwhelming Importance of Shaping the Far Future_](https://80000hours.org/wp-content/uploads/2022/01/Beckstead-Nick-On-the-Overwhelming-Importance-of-Shaping-the-Far-Future-better-formatting.pdf) by Nick Beckstead. It’s also discussed in [our podcast with Toby Ord](https://80000hours.org/articles/why-the-long-run-future-matters-more-than-anything-else-and-what-we-should-do-about-it).[^:`It’s also been argued that **even if** you take a person-affecting approach, you might still think that the future is more important than the present. This is because some people who are alive today might be able to live a very long time, and have much higher levels of welfare than they do today. Rather than create more people, your key concern should be to ensure that these possibilities are realised. So, even if you have a person-affecting view, the long-term value thesis might still hold.`]",
    ),
    (
        "future-generations:184",
        "What’s our position? As stated, we find the criticisms of the person-affecting view persuasive, so don’t find it a convincing response to the long-term value thesis. However, since many people hold something like the person-affecting view, we think it deserves some weight, and that means we should act as if we have somewhat greater obligations to help someone who’s already alive compared to someone who doesn’t exist yet. (This is an application of [moral uncertainty](https://80000hours.org/articles/moral-uncertainty/)).",
    ),
    (
        "future-generations:186",
        "Likewise, we think there might be other types of ethical reasons to have additional concern for the present. For instance, maybe the unique nature of injustice means we have _extra_ reasons to fight great injustices being perpetrated today. (Though there may also be reasons of justice to make sure the interests of future generations aren’t ignored.)",
    ),
    (
        "future-generations:188",
        "However, these reasons to place special value on the present need to be set against the potentially far greater amount of value in the future. How to do this is an extremely difficult question, and involves unsettled questions in the study of [moral uncertainty](https://80000hours.org/articles/moral-uncertainty/).",
    ),
    (
        "future-generations:190",
        "Trying to weigh this up, we think we should have far greater concern for the future, though we care more about the present generation than we would if we naively weighed up the numbers.",
    ),
    (
        "future-generations:192",
        "## So, is there much more value in the future?",
    ),
    (
        "future-generations:194",
        "We think the original argument survives the responses, and so, when it comes to doing good, our key concern is how the long-term unfolds.",
    ),
    (
        "future-generations:196",
        "That said, we’re still highly uncertain about these arguments. There’s a good chance we’ve missed a [crucial consideration](http://www.stafforini.com/blog/bostrom/) and this picture is wrong. These ideas are still new and have not been heavily studied. We’re also uncertain how to weigh the value of the future against other moral concerns given [moral uncertainty](https://concepts.effectivealtruism.org/concepts/moral-uncertainty/).",
    ),
    (
        "future-generations:198",
        "This makes us cautious to put _overwhelming_ value on the future, even if that’s what the raw numbers might imply. Instead, we see making the future go well as our key, but not only, moral concern.",
    ),
    (
        "future-generations:200",
        "We also place a great amount of value on learning more about these issues to refine our priorities, and we attach importance to many other moral demands.",
    ),
    (
        "future-generations:202",
        "## Can we actually influence the future?",
    ),
    (
        "future-generations:204",
        "You might be persuaded by these arguments, but believe they are irrelevant because we can’t significantly impact the future. It’s natural to think that the overall effects of our actions on the future are unknowable. If so, you might accept the long-term value thesis but reject longtermism, and instead believe that the best we can do is help people in the short term.",
    ),
    (
        "future-generations:206",
        "However, we think there are ways we can impact the future:",
    ),
    (
        "future-generations:208",
        "1. We can speed up processes that impact the future. Our economy tends to grow every year, and this suggests that if we make society wealthier today, this wealth will compound, making future people wealthier.",
    ),
    (
        "future-generations:210",
        "2. More significantly, we could precipitate the end of civilisation, perhaps through a nuclear war, run-away climate change, engineered pandemics, or other disasters. This would foreclose the possibility of all future value. It seems like there are things the current generation can do to increase or decrease the chance of extinction, as we cover in our [problem profiles](/problem-profiles/).",
    ),
    (
        "future-generations:212",
        "3. There might be other major, irreversible changes besides extinction that we can influence, which could either be positive or negative. For instance, if a totalitarian government came into power that couldn’t be overthrown, the badness of that government would be locked-in for a very long time. If we could decrease the chance of that happening, that would be very good, and vice versa. Alternatively, genetic engineering might make it possible to fundamentally change human values, and then these values would be passed down to every future generation. This could either be very good or very bad, depending on your moral views and how it was done.",
    ),
    (
        "future-generations:214",
        "4. Even if you’re not sure how to help the future, then your key aim could be to do _research_ to work it out. We’re uncertain about lots of ways to help people, but that doesn’t mean we shouldn’t try. This is part of [global priorities research](https://80000hours.org/problem-profiles/global-priorities-research/), and there are plenty of concrete questions to investigate.",
    ),
    (
        "future-generations:217",
        "\nYou can see more detail on the different ways to shape the future in Chapter 3 of [_On the Overwhelming Importance of Shaping the Far Future_](https://80000hours.org/wp-content/uploads/2022/01/Beckstead-Nick-On-the-Overwhelming-Importance-of-Shaping-the-Far-Future-better-formatting.pdf).",
    ),
    (
        "future-generations:219",
        "What’s the practical upshot of these possibilities?",
    ),
    (
        "future-generations:221",
        "If you think there’s a huge amount of value in the future, and there are not-ridiculously-small ways we can affect it, then these actions will be the highest-impact we can take. Nick Beckstead calls this the “future-shaping argument.”",
    ),
    (
        "future-generations:223",
        "In fact, it turns out that many of the ways to help future generations are also highly _neglected_. This is exactly what you’d expect — the present generation has a much greater interest in helping itself rather than improving the future.",
    ),
    (
        "future-generations:225",
        "Future generations can’t buy things, so lack economic power. They lack any political representation, and depend entirely on our generosity towards them. And because we never see the effects of our actions, even people who want to make a difference often neglect them. Within philanthropy, too, very little attention is paid to the interests of those who might live more than 100 years in the future.",
    ),
    (
        "future-generations:227",
        "This all suggests that outstanding opportunities to help may remain untaken, and that it would be reasonable for society to allocate significantly more attention to these issues.",
    ),
    (
        "future-generations:229",
        "You can read more about [objections to longtermism and responses in this article](https://www.effectivealtruism.org/articles/longtermism/#objections).",
    ),
    (
        "future-generations:231",
        "## What are the best ways to help future generations right now?",
    ),
    (
        "future-generations:233",
        "This is a topic for another article, but here is a very quick outline of our views.",
    ),
    (
        "future-generations:235",
        "One area that _doesn’t_ seem to be the top priority is speeding up progress. In terms of importance, [Beckstead argues in Chapter 3 of his thesis](https://rucore.libraries.rutgers.edu/rutgers-lib/40469/PDF/1/play/) (expanding on Nick Bostrom’s original paper, [Astronomical Waste](https://www.nickbostrom.com/astronomical/waste.html)) that from a long-term perspective, what matters most is where we end up, not how fast we get there, so speed-ups are less important than changes that alter the long-term trajectory.",
    ),
    (
        "future-generations:237",
        "Second, efforts to speed up progress are also far less neglected than other ways to help the future — the world spends about one trillion dollars a year on R&D, and a lack of neglect is what we should expect, since these discoveries also benefit the present generation.",
    ),
    (
        "future-generations:239",
        "Instead, what’s most important are “path changes” — actions that have the potential to shape the future over a very long timescale. (You could argue that speeding up progress will result in a path change, but the path change is where most of the value is.)",
    ),
    (
        "future-generations:241",
        "The main question is then which types of path change we should focus on?",
    ),
    (
        "future-generations:243",
        "The clearest example of a priority today seems to be reducing existential risks.",
    ),
    (
        "future-generations:245",
        "There is a small but real possibility that civilisation ends in the next century; and this would not only be terrible for the present generation, it would permanently remove the possibility of a good future. We need to get these risks down before we can focus on other ways of improving the future. What’s more, there are many concrete, highly neglected [proposals to reduce these risks](https://80000hours.org/articles/existential-risks/#what-can-be-done-about-these-risks).",
    ),
    (
        "future-generations:247",
        "Read more about [the arguments for focusing on existential risks](https://80000hours.org/articles/extinction-risk/), which apply even if you’re mainly focused on the present generation, but even more so if you accept the long-term value thesis.",
    ),
    (
        "future-generations:249",
        "Not everyone focused on longtermism thinks we should focus on directly reducing specific existential risks. Some think we should try to shape emerging technologies to increase the chance they go well (as well as reduce risks), others think we should focus on making society generally better able to navigate challenges (since perhaps we don’t know what the biggest risks are), and yet others think we should focus on building the resources of those concerned about longtermism in the future, so they’re in a better position to act compared to us — this is called [patient longtermism](https://80000hours.org/2020/08/the-emerging-school-of-patient-longtermism/). We discuss these options more in a [podcast with Benjamin Todd](https://80000hours.org/podcast/episodes/ben-todd-on-varieties-of-longtermism/).",
    ),
    (
        "future-generations:251",
        "We’re also deeply unsure about all of these suggestions, so our other focus is on [global priorities research](https://80000hours.org/problem-profiles/global-priorities-research/) to identify the best ways to help the future. This includes the question of whether there might be positive path changes to promote (sometimes called [“existential hope”](https://www.fhi.ox.ac.uk/Existential-risk-and-existential-hope.pdf)), as well as what negative risks are most important to avoid.",
    ),
    (
        "future-generations:253",
        "There might also be [crucial considerations](http://www.stafforini.com/blog/bostrom/) that might overturn longtermism. We think this research could have a significant impact on our priorities, and there’s also only a handful of researchers currently doing it.",
    ),
    (
        "future-generations:255",
        "The stakes facing our generation are much more than they first seem. Our actions might have the potential to bring about a far better world, or cut it short. Our key concern should be to ensure the future goes well.",
    ),
    (
        "future-generations:257",
        "<div class=\"well bg-gray-lighter margin-bottom margin-top padding-top-small padding-bottom-small\">",
    ),
    (
        "future-generations:259",
        "## Want to focus your career on the long-run future?",
    ),
    (
        "future-generations:261",
        "If you want to work on any issues essential to ensuring the future goes well, such as controlling nuclear weapons or shaping the development of artificial intelligence or biotechnology, you can speak to our team one-on-one.",
    ),
    (
        "future-generations:263",
        "We’ve helped hundreds of people choose an area to focus, make connections, and then find jobs and funding in these areas. If you’re already in one of these areas, we can help you maximise your impact within it.",
    ),
    (
        "future-generations:265",
        "<a href=\"https://80000hours.org/speak-with-us/?int_campaign=article__long-term-future\" title=\"\" class=\"btn btn-primary\">Speak to us</a>",
    ),
    (
        "",
        "",
    ),
    (
        "future-generations:269",
        "## Further reading",
    ),
    (
        "",
        "",
    ),
    (
        "future-generations:286",
        "<div class=\"tw--mt-6 tw--p-3 tw--pt-2 tw--bg-gray-lighter tw--rounded-md \">",
    ),
    (
        "future-generations:288",
        "### <a class=\"tw--text-off-black hover:tw--text-off-black hover:tw--no-underline focus:tw--text-off-black\" href=\"https://80000hours.org/articles/effective-altruism/\"> <small>Read next:\u{a0}</small> Effective altruism </a>",
    ),
    (
        "",
        "",
    ),
    (
        "future-generations:294",
        "<div><div class=\"tw--pb-3\">",
    ),
    (
        "future-generations:296",
        "Why some ways of doing good are much more effective than others, how we underestimate the size of the effect.",
    ),
    (
        "",
        "",
    ),
    (
        "future-generations:300",
        "<div>\n<a href=\"https://80000hours.org/articles/effective-altruism/\" class=\"btn btn-primary\">Continue →</a>\n</div>",
    ),
    (
        "",
        "",
    ),
    (
        "",
        "",
    ),
    (
        "",
        "",
    ),
    (
        "",
        "",
    ),
    (
        "future-generations:312",
        "### Enter your email and we’ll mail you a book (for free).",
    ),
    (
        "future-generations:314",
        "Join our newsletter and we’ll send you a free copy of _The Precipice_ — a book by philosopher Toby Ord about how to tackle the greatest threats facing humanity.",
    ),
    (
        "",
        "",
    ),
    (
        "",
        "",
    ),
    (
        "future-generations:320",
        "<div class=\"margin-top-large margin-bottom-large\">\n!social-share",
    ),
    (
        "",
        "",
    ),
    (
        "future-generations:325",
        "<footer><div class=\"margin-top margin-bottom-large\"><ul class=\"post-categories tags\"><li class=\"tags__tag\"><a href=\"https://80000hours.org/topic/causes/future-generations/\" title=\"View all posts on topic: Future Generations\" rel=\"category tag\">Future Generations</a></li><li class=\"tags__tag\"><a href=\"https://80000hours.org/topic/big-picture/longtermism/\" title=\"View all posts on topic: Longtermism\" rel=\"category tag\">Longtermism</a></li><li class=\"tags__tag\"><a href=\"https://80000hours.org/topic/big-picture/moral-philosophy/\" title=\"View all posts on topic: Moral philosophy\" rel=\"category tag\">Moral philosophy</a></li></ul></div></footer>",
    ),
]
[src/lib.rs:142] &key = "future-generations:031"
[src/lib.rs:142] &key = "future-generations:035"
[src/lib.rs:142] &key = "future-generations:036"
[src/lib.rs:142] &key = "future-generations:037"
[src/lib.rs:142] &key = "future-generations:038"
[src/lib.rs:142] &key = "future-generations:039"
[src/lib.rs:142] &key = "future-generations:077"
[src/lib.rs:142] &key = "future-generations:265"
[src/lib.rs:142] &key = "future-generations:269"
[src/lib.rs:142] &key = "future-generations:288"
[src/lib.rs:142] &key = "future-generations:296"
[src/lib.rs:142] &key = "future-generations:300"
[src/lib.rs:142] &key = "future-generations:301"
[src/lib.rs:142] &key = "future-generations:302"
[src/lib.rs:142] &key = "future-generations:303"
[src/lib.rs:142] &key = "future-generations:314"
[src/lib.rs:142] &key = "future-generations:315"
[src/lib.rs:142] &key = "future-generations:320"
